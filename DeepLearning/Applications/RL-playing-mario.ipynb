{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2b2e14-7530-497e-854f-4a1c8967166d",
   "metadata": {},
   "source": [
    "This notebook follows the official pytorch tutorial:\n",
    "[Mario-Playing RL Agent](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#train-a-mario-playing-rl-agent)\n",
    "\n",
    "Useful resources:\n",
    "- Model paper: [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "- Official repo of the code: [MadMario](https://github.com/yfeng997/MadMario)\n",
    "- Reinforcement Learning cheatsheet: [RL Cheatsheet](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5cd39e7-4318-445e-87bf-f4c620799dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import random, time, datetime, os, copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "import gym_super_mario_bros\n",
    "\n",
    "# NES Emulator for Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# suppress warning for now\n",
    "import warnings \n",
    "warnings.filterwarnings(action=\"once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c27410-11c5-45fe-bbf8-ac53f7682c0b",
   "metadata": {},
   "source": [
    "**Optimal Action-Value function** $Q^\\star(s, a)$: Gives the expected return if you start in state $s$, take arbitrary action $a$, and then for each future time step take the action that maximizes returns. $Q$ can be said to stand for the \"qaulity\" of the action in a state. We try to approximate this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b62e84e-c0e3-4783-a475-4957f864862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saliei/Documents/penvs/dpl/lib/python3.10/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-1-1-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "# limit the action-spcae to\n",
    "#    0. walk right\n",
    "#    1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", 'A']])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb49793-7295-40b9-bcf9-09b15d229203",
   "metadata": {},
   "source": [
    "preprocess environment with different **wrappers**: `GrayScaleObseravtion`, `ResieObservation`, `SkipFrame`, and `FrameStack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98ee0495-4cdf-4efd-9cac-0668111ea837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9477a16-3e12-4a87-b8dd-bd364e7deb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "        \n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8e0714e-8be7-42a2-8b81-89adef2f6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "             self.shape = tuple(shape)\n",
    "                \n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=225, shape=obs_shape, dtype=np.uint8)\n",
    "        \n",
    "    def observation(self,  observation):\n",
    "        transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 225)])\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "46a136c0-dff3-48c1-b587-d9752e047588",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc39188-b2d1-4846-97e5-2f304cb77eb7",
   "metadata": {},
   "source": [
    "Our agent should be able to:\n",
    "- **Act**\n",
    "- **Remember**\n",
    "- **Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b0576b0-e0ad-4496-a805-95756cfcb6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        \n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "            \n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "        \n",
    "        self.save_every = 5e5\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action and update value of step.\n",
    "        \n",
    "        Inputs:\n",
    "            state(LazyFrame): A single observation of the current state, dimension is (state_dim).\n",
    "            \n",
    "        Outputs:\n",
    "            action_idx(int): An integer representing which action Marion will perform.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            \n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "            \n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate  = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        \n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d35743f-4592-4f05-a33d-24a8811c43b2",
   "metadata": {},
   "source": [
    "`cache()`: store `experience` and to memory pof the agent.\n",
    "\n",
    "`recall()`: randomly sample a batch of experiences from agent memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2529194-c5a9-4ad2-8c19-d2487193a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        \n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"Store the experience to self.memory (reply buffer)\n",
    "        \n",
    "        Inputs:\n",
    "            state(LazyFrame)\n",
    "            next_state(LazyFrame)\n",
    "            action(int)\n",
    "            reward(float)\n",
    "            done(bool)\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            state      = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action     = torch.tensor([action]).cuda()\n",
    "            reward     = torch.tensor([reward]).cuda()\n",
    "            done       = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state      = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action     = torch.tensor([action])\n",
    "            reward     = torch.tensor([reward])\n",
    "            done       = torch.tensor([done])\n",
    "            \n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "        \n",
    "    def recall(self):\n",
    "        \"\"\"Retrieve a batch of experiences from memory\"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stach, zip(*batch))\n",
    "        return state, next_state, action_squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e765b-ef99-4bd4-a1d3-c878b40f46be",
   "metadata": {},
   "source": [
    "Our agent uses [DDQN algorithm](https://arxiv.org/pdf/1509.06461) under the hood. DDQN uses two ConvNets - $Q_{\\text{online}}$ and $Q_{\\text{target}}$ that independently approximate the optimal action-value function/\n",
    "\n",
    "We share feature generator `features` across $Q_{\\text{online}}$ and $Q_{\\text{target}}$, but maintain seperate FC classifiers for each $\\theta_{\\text{target}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee9f94cd-5a21-4242-a174-975b5b633377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"Mini cnn structure.\n",
    "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "        \n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "            \n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "        \n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        \n",
    "        # freeze theta_target\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    def forward(self, input, model):\n",
    "        if model   == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ed8e72a-e740-4702-966b-410fd375f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[np.arange(0, self.batch_size), action]\n",
    "        return current_Q\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        base_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[np.arange(0, self.batch_size), best_action]\n",
    "        return (reward + (1-done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6849754d-135a-42dd-be00-b4576d5a2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "        \n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2c9f9d5-9e46-4492-8551-c7fa7b81ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\")\n",
    "        torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration), save_path)\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ccdc7ce7-e89b-49e9-bb45-c42453f90492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4 # min experiences before training\n",
    "        self.learn_every = 3 # no of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4 # no of experiences between Q_target and Q_online sync\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "            \n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "            \n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "        \n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        \n",
    "        td_est = self.td_estimate(state, action)\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "        \n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "        \n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1835a965-c1bb-46bc-ba5b-46dd486bf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "        \n",
    "        self.ep_rewards    = []\n",
    "        self.ep_lengths    = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs     = []\n",
    "        \n",
    "        self.moving_avg_ep_rewards    = []\n",
    "        self.moving_avg_ep_lengths    = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs     = []\n",
    "        \n",
    "        self.init_episode()\n",
    "        \n",
    "        self.record_time = time.time()\n",
    "        \n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "            \n",
    "    def log_episode(self):\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        \n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q    = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "        \n",
    "        self.init_episode()\n",
    "        \n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "        \n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        \n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "        \n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "        \n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\",\n",
    "            end=\"\\r\"\n",
    "        )\n",
    "        \n",
    "        with open(self.save_log, 'a') as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "            \n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "117f1b9a-72bd-4243-8a41-a9c01445e1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n",
      "Episode 0 - Step 527 - Epsilon 0.9998682586621644 - Mean Reward 1.07 - Mean Length 1.0 - Mean Q Value 0.0 - Time Delta 0.736 - Time 2022-04-14T17:14:331\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56949/128938856.py:31: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done       = torch.tensor([done])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        # run agent on the state\n",
    "        action = mario.act(state)\n",
    "        # agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "        # learn\n",
    "        q, loss = mario.learn()\n",
    "        # logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "        # update state\n",
    "        state = next_state\n",
    "        \n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "            \n",
    "        logger.log_episode()\n",
    "        \n",
    "        if e % 20 == 0:\n",
    "            logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26146e94-dbd6-45d3-a06e-b77103230aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
