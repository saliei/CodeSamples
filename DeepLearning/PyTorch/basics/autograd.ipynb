{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cf8aaf-a3d3-495f-af96-b09a06a71b40",
   "metadata": {},
   "source": [
    "Consider one-layer neural network with input `x`, parameters `w` and `b`, and some loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0535d16-9d9a-4c0e-a560-3cac807107a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2c7340-99ba-42ed-b702-c491e2b2cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([1., 1., 1., 1., 1.])\n",
      "y:\n",
      " tensor([0., 0., 0.])\n",
      "w:\n",
      " tensor([[-0.1101,  0.3991,  0.8016],\n",
      "        [-0.1892,  0.7182,  1.2419],\n",
      "        [ 1.0189, -0.5179,  1.3606],\n",
      "        [ 1.1578,  0.4373,  0.6054],\n",
      "        [-1.2938, -0.0192, -0.9676]], requires_grad=True)\n",
      "b:\n",
      " tensor([ 0.3753,  0.6999, -0.2055], requires_grad=True)\n",
      "z:\n",
      " tensor([0.9589, 1.7175, 2.8364], grad_fn=<AddBackward0>)\n",
      "loss:\n",
      " 2.0197770595550537\n",
      "gradient function for z:\n",
      " <AddBackward0 object at 0x7f718d5c61a0>\n",
      "gradient funtion for loss:\n",
      " <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f718d5c5510>\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "print(f\"x:\\n {x}\")\n",
    "print(f\"y:\\n {y}\")\n",
    "print(f\"w:\\n {w}\")\n",
    "print(f\"b:\\n {b}\")\n",
    "print(f\"z:\\n {z}\")\n",
    "print(f\"loss:\\n {loss}\")\n",
    "\n",
    "print(f\"gradient function for z:\\n {z.grad_fn}\")\n",
    "print(f\"gradient funtion for loss:\\n {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d877c4-124a-4763-9aa5-5f7193733eda",
   "metadata": {},
   "source": [
    "> To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function \n",
    "with respect to parameters, namely we need $\\frac{\\partial_{\\text{loss}}}{\\partial\\omega}$ and $\\frac{\\partial_{\\text{loss}}}{\\partial b}$ under some fixed values of `x` and `y`. To compute those derivatives, we call `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20477df-74e2-450b-951a-cf66982ccb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad:\n",
      " tensor([[0.2410, 0.2826, 0.3149],\n",
      "        [0.2410, 0.2826, 0.3149],\n",
      "        [0.2410, 0.2826, 0.3149],\n",
      "        [0.2410, 0.2826, 0.3149],\n",
      "        [0.2410, 0.2826, 0.3149]])\n",
      "b.grad:\n",
      " tensor([0.2410, 0.2826, 0.3149])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(f\"w.grad:\\n {w.grad}\")\n",
    "print(f\"b.grad:\\n {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afad35c1-224e-4e89-9c1b-5298b7c59fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z requires grad: True\n",
      "z requires grad: False\n",
      "z requires grad: True\n",
      "z_det requires grad: False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(f\"z requires grad: {z.requires_grad}\")\n",
    "\n",
    "# disable gradient tracking\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(f\"z requires grad: {z.requires_grad}\")\n",
    "\n",
    "# we can also use detach() method\n",
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(f\"z requires grad: {z.requires_grad}\")\n",
    "print(f\"z_det requires grad: {z_det.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234300c5-d066-43e4-ba28-ab9ef83a09d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
